experiment:
  preprocessings: ["identic"]  # "asinh", "identic", "log"
  normalisation: True  # False
  supervised:
    models:
#      active: ['sgdr', 'tr', 'poly']  # 'mlp', 'nb', 'lr', 'sgdr', 'tr', 'poly'
      active: ['mlp']
      configs:
        mlp:
          hidden_layer_sizes: (100,)
          activation: 'relu'
          solver: 'adam'
          alpha: 0.0001
          batch_size: 'auto'
          learning_rate: 'constant'
          learning_rate_init: 0.001
          power_t: 0.5
          max_iter: 6000
          shuffle: True
          random_state: null
          tol: 1e-4
          verbose: False
          warm_start: False
          momentum: 0.9
          nesterovs_momentum: True
          early_stopping: False
          validation_fraction: 0.1
          beta_1: 0.9
          beta_2: 0.999
          epsilon: 1e-8
          n_iter_no_change: 10
          max_fun: 15000

        nb:
          alpha: 1.0
          fit_prior: True
          class_prior: null
          min_categories: null

        lr:
          penalty: 'l2'
          dual: False
          tol: 1e-4
          C: 1.0
          fit_intercept: True
          intercept_scaling: 1
          class_weight: null
          random_state: null
          solver: 'lbfgs'
          max_iter: 6000
          multi_class: 'auto'
          verbose: 0
          warm_start: False
          n_jobs: null
          l1_ratio: null

        sgdr:
          loss: 'squared_loss'  # later versions implement 'squared_error'
          penalty: 'l2'
          alpha: 0.0001
          l1_ratio: 0.15
          fit_intercept: True
          max_iter: 6000
          tol: 0.001  # 1e-3
          shuffle: True
          verbose: 0
          epsilon: 0.1  # DEFAULT_EPSILON
          random_state: null
          learning_rate: 'invscaling'
          eta0: 0.01
          power_t: 0.25
          early_stopping: False
          validation_fraction: 0.1
          n_iter_no_change: 5
          warm_start: False
          average: False

        tr:
          power: 0.0
          alpha: 1.0
          fit_intercept: True
          link: 'auto'
          max_iter: 6000
          tol: 0.0001  # 1e-4
          warm_start: False
          verbose: 0

        poly:
          degree: 3  # interaction_only=False, include_bias=True, order='C'
  unsupervised:
    fresh_start: False
    savefig: False
    models:
      active: ['umap']  # 't-sne', 'umap'
      configs:
        umap:
          n_neighbors: 15
          n_components: 2
          metric: "euclidean"
          metric_kwds: null
          output_metric: "euclidean"
          output_metric_kwds: null
          n_epochs: null
          learning_rate: 1.0
          init: "spectral"
          min_dist: 0.1
          spread: 1.0
          low_memory: True
          n_jobs: -1
          set_op_mix_ratio: 1.0
          local_connectivity: 1.0
          repulsion_strength: 1.0
          negative_sample_rate: 5
          transform_queue_size: 4.0
          a: null
          b: null
          random_state: null
          angular_rp_forest: False
          target_n_neighbors: -1
          target_metric: "categorical"
          target_metric_kwds: null
          target_weight: 0.5
          transform_seed: 42
          transform_mode: "embedding"
          force_approximation_algorithm: False
          verbose: False
          unique: False
          densmap: False
          dens_lambda: 2.0
          dens_frac: 0.3
          dens_var_shift: 0.1
          output_dens: False
          disconnection_distance: null

        t-sne:
          n_components: 2
          perplexity: 15.0  # 30.0
          early_exaggeration: 10.0  # 12.0
          learning_rate: 150  # 200.0
          n_iter: 2000  # 1000
          n_iter_without_progress: 300
          min_grad_norm: 1e-7
          metric: "euclidean"
          init: "random"
          verbose: 0
          random_state: null
          method: 'barnes_hut'
          angle: 0.5
          n_jobs: null
          square_distances: 'legacy'
